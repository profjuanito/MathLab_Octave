

Exercício 1

- entrar em Octave
- Mudar de diretorio
  cd Desktop/MachineLearning/MachineLearningStandford/Octave/ex1-ex8-matlab/ex1-octave/
- editar usando Xcode  warmUpExercise.m  
  A=eye(5);    crib uma matriz identidad 
  salvar  
- executar warmUpExercise.m usando 
- submit()

- leer os dados ex1data1.txt usando 
  load ex1data1.txt
  size(ex1data1)   most quants dados tem resposta aos = 97 2
  whos 
   Attr Name          Size                     Bytes  Class
   ==== ====          ====                     =====  =====
        ans           1x2                         16  double
        ex1data1     97x2                       1552  double
   Total is 196 elements using 1568 bytes

   x = ex1data1(:, 1);   coluna 1 na variável x
   max(x);           valor máximo x            ans = 22.203
   y = ex1data1(:, 2);   coluna 2 na variável y
   max(y);           valor máximo y            ans = 24.147
   m = length(y);    nro de dados em y
   m                                           ans = 97   
   mx = length(x);   nro de dados em x
                                   
 
- editar plotData.m
   plot(x, y, 'rx', 'MarkerSize', 10); % Plot the data
   ylabel('Profit in $10,000s'); % Set the y−axis label
   xlabel('Population of City in 10,000s'); % Set the x−axis label

- executar plotData
  plotData()

- implementar gradient descend com os parâmetros iniciais
  x = [ones(m, 1), ex1data1(:,1)]; % Add a column of ones to x  97 filas , 2 colunas    theta = zeros(2, 1); % initialize fitting parameters       2 filas , 1 coluna   iterations = 1500;  alpha = 0.01;

- editar gradientDescent
  mudar num_iters por iterations na cabeceira da função
     

Programa
********************
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
m
J_history = zeros(2, 1);

for iter = 1:num_iters

    % ====================== YOUR CODE HERE ======================
    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCost) and gradient here.
    %

    temp1 = 0;
    temp2 = 0;
    for itera=1:m
        %  itera
        temp1 = temp1 + ( ( theta(1)*X(itera,1) + theta(2)*X(itera,2) ) - y(itera) ) * X(itera,1);
        %printf("temp1 =");
        %  temp1
        temp2 = temp2 + ( ( theta(1)*X(itera,1) + theta(2)*X(itera,2) ) - y(itera) ) * X(itera,2);
        %printf("temp2 =");
        %  temp2
    end
    theta(1) = theta(1) - (alpha/m)*temp1;
    theta(2) = theta(2) - (alpha/m)*temp2;
    printf("theta(1) =");
    theta(1)
    printf("theta(2) =");
    theta(2)
    % ============================================================

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
    printf("J_history ")
    iter
    printf("  =  ")
    J_history(iter)
end

end



Program paused. Press enter to continue.
Normalizing Features ...
Running gradient descent ...
Theta computed from gradient descent:
 338658.249249
 104127.515597
 -172.205334

Predicted price of a 1650 sq-ft, 3 br house (using gradient descent):
 $292748.085232
Program paused. Press enter to continue.
Solving with normal equations...
Theta computed from the normal equations:
 89597.909542
 139.210674
 -8738.019112

Predicted price of a 1650 sq-ft, 3 br house (using normal equations):
 $293081.464335

